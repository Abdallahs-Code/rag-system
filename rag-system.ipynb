{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14666779,"sourceType":"datasetVersion","datasetId":9369942}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sentence-transformers PyPDF2 faiss-cpu -U bitsandbytes accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport faiss\nimport torch\nimport numpy as np\nimport unicodedata\nfrom PyPDF2 import PdfReader\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path):\n    reader = PdfReader(pdf_path)\n    text = \"\"\n    for page in reader.pages:\n        page_text = page.extract_text()\n        if page_text:\n            text += page_text + \"\\n\"\n    text = unicodedata.normalize(\"NFKC\", text)\n    text = re.sub(r\"[•◦▪▫‣⁃∙◆◇■□●○]\", \" \", text)\n    text = re.sub(r\"[\\x00-\\x09\\x0B-\\x1F\\x7F]\", \" \", text)\n    text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_text(text, chunk_size=5, overlap=1, min_chars=50):\n    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n    chunks = []\n    start = 0\n    while start < len(lines):\n        end = start + chunk_size\n        chunk = \" \".join(lines[start:end])\n        if len(chunk) >= min_chars and any(c.isalpha() for c in chunk):\n            chunks.append(chunk)\n        start += (chunk_size - overlap)\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def embed_chunks(chunks, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n    model = SentenceTransformer(model_name)\n    embeddings = model.encode(chunks, convert_to_numpy=True)\n    return model, embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_faiss_index(embeddings):\n    dim = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dim)\n    index.add(embeddings)\n    return index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def search_index(query, model, index, chunks, k=5):\n    query_embedding = model.encode([query], convert_to_numpy=True)\n    distances, indices = index.search(query_embedding, k)\n    return [chunks[i] for i in indices[0]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(prompt, max_new_tokens=80, num_return_sequences=1):\n    device = model.device\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    enc = tokenizer.apply_chat_template(\n        messages,\n        return_tensors=\"pt\",\n        add_generation_prompt=True\n    )\n    input_ids = enc.to(device)\n    attention_mask = torch.ones_like(input_ids, device=device)\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.95,\n            use_cache=False,              \n            pad_token_id=tokenizer.eos_token_id\n        )\n    generated_ids = outputs[0][input_ids.shape[-1]:]\n    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    return [answer]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(question, chunks):\n    context_text = \"\\n\".join(c.strip() for c in chunks if c.strip())\n\n    prompt = (\n        \"Answer the question using only the provided context.\\n\\n\"\n        f\"Context:\\n{context_text}\\n\\n\"\n        f\"Question:\\n{question}\\n\\n\"\n        \"Answer in 2–3 sentences.\"\n    )\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_pdf(directory=\"/kaggle/input\"):\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(\".pdf\"):\n                return os.path.join(root, file)\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pdf_path = None\nchunks = []\nmodel_embeddings = None\nembeddings = None\nindex = None\nloaded = False\n\nprint(\"Please upload your PDF. I'll be here to answer your questions!\\n\")\n\nwhile True:\n    question = input().strip()\n    if not loaded:\n        pdf_path = find_pdf()\n        if pdf_path is None:\n            print(\"No PDF detected. Please upload a PDF so I can help you.\")\n            continue\n        text = extract_text_from_pdf(pdf_path)\n        chunks = chunk_text(text, chunk_size=5, overlap=1)\n        model_embeddings, embeddings = embed_chunks(chunks)\n        index = create_faiss_index(embeddings)\n        loaded = True\n    top_chunks = search_index(question, model_embeddings, index, chunks, k=3)\n    prompt = build_prompt(question, top_chunks)\n    llm_outputs = generate_text(prompt, max_new_tokens=50, num_return_sequences=1)\n    answer = llm_outputs[0]\n    print(f\"{answer}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}