{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu langchain langchain-community langchain-core -qU langchain-huggingface","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n# Surpass warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport torch\nimport unicodedata\nfrom langchain_core.documents import Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path):\n    loader = PyPDFLoader(pdf_path)\n    documents = loader.load()\n    cleaned_docs = []\n    for doc in documents:\n        clean_text = doc.page_content\n        bullets = '•◦▪▫‣⁃∙◆◇■□●○'\n        for bullet in bullets:\n            clean_text = clean_text.replace(bullet, '')\n        cleaned_doc = Document(\n            page_content=clean_text,\n            metadata=doc.metadata\n        )\n        cleaned_docs.append(cleaned_doc)\n    return cleaned_docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk(documents, chunk_size=1000, chunk_overlap=200):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        separators=[\n            \"\\n\\n\",  \n            \"\\n\",    \n            \". \",    \n            \" \",     \n            \"\"       \n        ]\n    )\n    chunks = text_splitter.split_documents(documents)\n    for chunk in chunks:\n        lines = chunk.page_content.split('\\n')\n        non_empty_lines = [line for line in lines if line.strip()]\n        chunk.page_content = '\\n'.join(non_empty_lines)\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_faiss_index(chunks):\n    embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n    embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n    index = FAISS.from_documents(chunks, embedding)\n    return index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def search_index(query, index, k=3):\n    docs = index.similarity_search(query, k)\n    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n    return context","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(prompt, max_new_tokens=100, num_return_sequences=1):\n    device = model.device\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    enc = tokenizer.apply_chat_template(\n        messages,\n        return_tensors=\"pt\",\n        add_generation_prompt=True\n    )\n    input_ids = enc.to(device)\n    attention_mask = torch.ones_like(input_ids, device=device)\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.95,           \n            pad_token_id=tokenizer.eos_token_id\n        )\n    generated_ids = outputs[0][input_ids.shape[-1]:]\n    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    return [answer]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(question, chunks):\n    context_text = \"\\n\".join(c.strip() for c in chunks if c.strip())\n\n    prompt = (\n        \"Answer the question using only the provided context.\\n\\n\"\n        f\"Context:\\n{context_text}\\n\\n\"\n        f\"Question:\\n{question}\\n\\n\"\n        \"Answer in 2–3 sentences.\"\n    )\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_pdf(directory=\"/kaggle/input\"):\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(\".pdf\"):\n                return os.path.join(root, file)\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    dtype=torch.float16,\n    device_map=\"auto\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Hi, I'll be here to answer your questions!\")\nlast_speaker = \"ai\"\n\npdf_path = find_pdf()\n\nwhile pdf_path is None:\n    if last_speaker != \"ai\":\n        print()\n    print(\"Please upload the pdf and notify me with 'done'.\")\n    last_speaker = \"ai\"\n    while True:\n        if last_speaker != \"user\":\n            print()\n        answer = input(\">\").strip().lower()\n        last_speaker = \"user\"\n        if answer == \"done\":\n            break\n        if last_speaker != \"ai\":\n            print()\n        print(\"Please ensure on me with 'done'.\")\n        last_speaker = \"ai\"\n    pdf_path = find_pdf()\n    if pdf_path is None:\n        if last_speaker != \"ai\":\n            print()\n        print(\"Didn't find any pdf. \", end='')\n        last_speaker = \"ai\"\n\nif last_speaker != \"ai\":\n    print()\nprint(\"Processing your document...\")\nlast_speaker = \"ai\"\npdf_path = find_pdf()\ndocuments = extract_text_from_pdf(pdf_path)\nchunks = chunk(documents)\nindex = create_faiss_index(chunks)\nif last_speaker != \"ai\":\n    print()\nprint(\"Document processed, ask me anything :)\")\nlast_speaker = \"ai\"\n\nwhile True:\n    if last_speaker != \"user\":\n        print()\n    question = input(\">\").strip()\n    last_speaker = \"user\"\n    context = search_index(question, index, k=2)\n    prompt = build_prompt(question, [context])\n    llm_outputs = generate_text(prompt, max_new_tokens=300, num_return_sequences=1)\n    answer = llm_outputs[0]\n    if last_speaker != \"ai\":\n        print()\n    print(f\"{answer}\")\n    last_speaker = \"ai\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}