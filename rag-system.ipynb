{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14630577,"sourceType":"datasetVersion","datasetId":9345873}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sentence-transformers PyPDF2 faiss-cpu nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\nimport torch\nimport numpy as np\nfrom PyPDF2 import PdfReader\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path):\n    reader = PdfReader(pdf_path)\n    full_text = \"\"\n    for page in reader.pages:\n        full_text += page.extract_text() + \"\\n\"\n    return full_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_text(text, chunk_size=3, overlap=1):\n    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n    chunks = []\n    start = 0\n    while start < len(lines):\n        end = start + chunk_size\n        chunk = \" \".join(lines[start:end])\n        chunks.append(chunk)\n        start += (chunk_size - overlap)\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def embed_chunks(chunks, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n    model = SentenceTransformer(model_name)\n    embeddings = model.encode(chunks, convert_to_numpy=True)\n    return model, embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_faiss_index(embeddings):\n    dim = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dim)\n    index.add(embeddings)\n    return index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def search_index(query, model, index, chunks, k=5):\n    query_embedding = model.encode([query], convert_to_numpy=True)\n    distances, indices = index.search(query_embedding, k)\n    return [chunks[i] for i in indices[0]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(prompt, max_new_tokens=100, num_return_sequences=1):\n    device = next(model.parameters()).device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        num_return_sequences=num_return_sequences,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    answers = []\n    for output in outputs:\n        generated_ids = output[inputs[\"input_ids\"].shape[-1]:]\n        answer = tokenizer.decode(generated_ids, skip_special_tokens=True)\n        answers.append(answer)\n    return answers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(question, chunks):\n    context_text = \"\\n\".join(chunks)\n    prompt = (\n        f\"You are a helpful assistant providing information based on the document provided.\\n\"\n        f\"Use the context below to answer the question.\\n\\n\"\n        f\"Context:\\n{context_text}\\n\\n\"\n        f\"Question: {question}\\n\"\n        f\"Answer the question, and at the end, you may invite the user to ask further questions if appropriate.\"\n    )\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/tips-hindawi-pdf/Tips Hindawi University Info.pdf\"\ntext = extract_text_from_pdf(pdf_path)\nchunks = chunk_text(text, chunk_size=3, overlap=1)\nmodel_embeddings, embeddings = embed_chunks(chunks)\nindex = create_faiss_index(embeddings)\nmodel_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Hello! I can help you explore your document. Ask me anything about it!\\n\")\nwhile True:\n    question = input(\"> \")\n    top_chunks = search_index(question, model_embeddings, index, chunks, k=3)\n    prompt = build_prompt(question, top_chunks)\n    llm_outputs = generate_text(prompt, max_new_tokens=200, num_return_sequences=1)\n    answer = llm_outputs[0]\n    print(f\"{answer}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}